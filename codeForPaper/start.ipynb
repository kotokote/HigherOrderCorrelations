{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This file is only looking at the roster plots for old organoid data\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import h5py\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.signal import correlate, gaussian\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_FILES = [\n",
    "    # \"2950_spike_mat_or_rand\", #26\n",
    "    # \"2953_spike_mat_or_rand\", #Tal paper\n",
    "    # \"2957_spike_mat_or_rand\",\n",
    "    # \"5116_spike_mat_or_rand\",\n",
    "    # \"M1S1_t_spk_mat_sorted\",\n",
    "    # \"M1S2_t_spk_mat_sorted\",\n",
    "    # \"M2S1_t_spk_mat_sorted\",\n",
    "    # \"M2S2_t_spk_mat_sorted\",\n",
    "    # \"M3S1_t_spk_mat_sorted\",\n",
    "    # \"M3S2_t_spk_mat_sorted\",\n",
    "    \"2953_t_spk_mat_sorted\",\n",
    "    \"2957_t_spk_mat_sorted\",\n",
    "    \"5116_t_spk_mat_sorted\",\n",
    "    \"O5_t_spk_mat_sorted\",\n",
    "    \"O6_t_spk_mat_sorted\",\n",
    "    # \"UCSC_mouse_Pasca_23129\",\n",
    "    # \"UCSC_mouse_Pasca_23149\",\n",
    "    # \"UCSC_mouse_Pasca_23179\",\n",
    "    # \"UCSC_mouse/240410/23129/t_spk_mat_sorted\",\n",
    "    # \"UCSC_mouse/240410/23179/t_spk_mat_sorted\",\n",
    "    # \"UCSC_mouse/240517/23124/t_spk_mat_sorted\",\n",
    "    # \"UCSC_mouse/240521/23137/t_spk_mat_sorted\",\n",
    "    # \"UCSC_mouse/240524/23150/t_spk_mat_sorted\",\n",
    "    # \"UCSC_mouse/240524/23178/t_spk_mat_sorted\",\n",
    "    # \"UCSC_mouse/240611/23141/t_spk_mat_sorted\",\n",
    "    # \"UCSC_mouse/240618/23150/t_spk_mat_sorted\",\n",
    "    # \"UCSC_mouse/240628/23120/t_spk_mat_sorted\",\n",
    "    # \"orgA-5run-predrug\",\n",
    "    # \"orgA-5run-predrug\",\n",
    "    # \"orgA-5run-predrug\",\n",
    "    # \"kilosort/000048\",\n",
    "    # \"kilosort/000049\",\n",
    "    # \"kilosort/000050\",\n",
    "]\n",
    "# Load your .mat file\n",
    "\n",
    "\"\"\"\n",
    "# Raster plots for 3 min of recording\n",
    "\n",
    "# Parameters:\n",
    "# - spike_times: 2D array-like, where each row represents a time unit and each column a neuron.\n",
    "# - units: int, total number of time units.\n",
    "# - unit_duration_ms: int, duration of each time unit in milliseconds.\n",
    "# - total_minutes: int, total time span in minutes.\n",
    "# \"\"\"\n",
    "\n",
    "fig, axs = plt.subplots((len(ALL_FILES) + 1) // 2, 2, figsize=(12, len(ALL_FILES) * 3))\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "for idx, fn in enumerate(ALL_FILES):\n",
    "    try:\n",
    "        data = loadmat(f\"{fn}.mat\")\n",
    "        t_spk_mat = data['t_spk_mat']\n",
    "    except:\n",
    "        data = h5py.File(f\"{fn}.mat\")\n",
    "        t_spk_mat = np.array(data['t_spk_mat']).T\n",
    "\n",
    "    # Plotting the raster plot and average spikes per unit\n",
    "    ax = axs[idx // 2, idx % 2]\n",
    "    spike_indices = t_spk_mat[:10*1000].nonzero()\n",
    "    ax.scatter(spike_indices[0] / 1000.0, spike_indices[1], marker='|', s=10)\n",
    "    ax.set_xlabel('Time (seconds)', fontsize=20)\n",
    "    ax.set_ylabel('Sorted unit ID', fontsize=20)\n",
    "    # ax.set_title(f\"Raster Plot for {fn}\")\n",
    "    ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=16)\n",
    "\n",
    "    avg_spikes = np.sum(t_spk_mat, axis=1)[:10*1000]\n",
    "    avg_spikes = avg_spikes.astype(float)\n",
    "    avg_spikes_smooth = gaussian_filter1d(avg_spikes, 100)\n",
    "\n",
    "    ax2 = ax.twinx()\n",
    "    xs = np.linspace(0, len(avg_spikes), len(avg_spikes)) / 1000.0\n",
    "    ax2.plot(xs, avg_spikes_smooth, color='red', linewidth=2)\n",
    "    ax2.yaxis.label.set_color('red')\n",
    "    ax2.set_ylabel('Population rate, kHz', fontsize=20)\n",
    "    ax2.spines['right'].set_color('red')\n",
    "    ax2.spines['right'].set_linewidth(2)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=18)\n",
    "    ax2.tick_params(axis='both', which='minor', labelsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots((len(ALL_FILES) + 1) // 2, 2, figsize=(12, len(ALL_FILES) * 3))\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "for idx, fn in enumerate(ALL_FILES):\n",
    "    try:\n",
    "        data = loadmat(f\"{fn}.mat\")\n",
    "        t_spk_mat = data['t_spk_mat']\n",
    "    except:\n",
    "        data = h5py.File(f\"{fn}.mat\")\n",
    "        t_spk_mat = np.array(data['t_spk_mat']).T\n",
    "\n",
    "    neuron = t_spk_mat[:, 0].astype(np.float32)\n",
    "    spike_indices = neuron[:10*1000]\n",
    "    xs = np.linspace(0, len(spike_indices), len(spike_indices)) / 1000.0\n",
    "    spike_indices_gaussian = gaussian_filter1d(spike_indices, 100)\n",
    "\n",
    "\n",
    "    ax = axs[idx // 2, idx % 2]\n",
    "    ax.plot(xs, spike_indices_gaussian, label='Gaussian', color='red')\n",
    "    ax.set_xlabel('Time (seconds)', fontsize=20)\n",
    "    ax.set_ylabel('Instantaneous firing rate', color='red', fontsize=20)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=16)\n",
    "    # ax.set_title(f\"Raster Plot for {fn}\")\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(xs, spike_indices, label='Original', alpha=0.5)\n",
    "    ax2.set_ylabel('Neuron Activity', fontsize=20)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=18)\n",
    "    ax2.tick_params(axis='both', which='minor', labelsize=16)\n",
    "    \n",
    "    fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "  \n",
    "    # plt.plot(xs, spike_indices)\n",
    "    #.nonzero()\n",
    "    # print(spike_indices)\n",
    "    # plt.scatter(spike_indices[0] / 1000.0, spike_indices[1], marker='|', s=10)\n",
    "    # plt.set_xlabel('Time (seconds)')\n",
    "    # ax.set_ylabel('Sorted unit ID')\n",
    "    # ax.set_title(f\"Raster Plot for {fn}\")\n",
    "   \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (no window) In order to choose threshold for the correlations, we need to make a histogram for each of the organoids data sets\n",
    "ALL_FILES = [\n",
    "    \"2950_spike_mat_or_rand\", #26\n",
    "    \"2953_spike_mat_or_rand\", #Tal paper\n",
    "    \"2957_spike_mat_or_rand\",\n",
    "    \"5116_spike_mat_or_rand\",\n",
    "]\n",
    "\n",
    "We want to keep the tail representing the highest correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from scipy.stats import powerlaw\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "for lag in [0, 10, 20]:\n",
    "    fig, axs = plt.subplots((len(ALL_FILES) + 1) // 2, 2, figsize=(12, len(ALL_FILES) * 3), constrained_layout=True)\n",
    "    fig.suptitle(f\"Correlation distribution for {lag} ms lag\")\n",
    "\n",
    "    fig_delta, axs_delta = plt.subplots((len(ALL_FILES) + 1) // 2, 2, figsize=(12, len(ALL_FILES) * 3), constrained_layout=True)\n",
    "    fig_delta.suptitle(f\"Correlation distributions delta for {lag} ms lag\")\n",
    "    for idx, fn in enumerate(ALL_FILES):\n",
    "        with open(f\"processed/{fn}_lag_window_0.pkl\", \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            data_all = data[\"all\"]\n",
    "        with open(f\"processed/{fn}_lag_window_0_shuffle.pkl\", \"rb\") as f:\n",
    "            data_shuffled = pickle.load(f)\n",
    "            data_shuffled_all = data_shuffled[\"all\"]\n",
    "        ax = axs[idx // 2, idx % 2]\n",
    "        ax_delta = axs_delta[idx // 2, idx % 2]\n",
    "        \n",
    "        idx = np.triu_indices(data_all[\"corr\"].shape[0], 1)\n",
    "        mask = data_all[\"corr\"][idx]> 0.03\n",
    "        mask_shuffled = data_shuffled_all[\"corr\"][idx]> 0.03\n",
    "        data_masked = data_all[\"corr\"][idx][mask]\n",
    "        data_masked_shuffled = data_shuffled_all[\"corr\"][idx][mask_shuffled]\n",
    "        bins = np.linspace(0, 1, 50)\n",
    "\n",
    "        hist, _, _ = ax.hist(data_masked, bins=bins, alpha=0.5, label=\"Original\", color='blue')\n",
    "        hist_shuffled, _, _ = ax.hist(data_masked_shuffled, bins=bins, alpha=0.5, label=\"Shuffled\", color='red')\n",
    "\n",
    "        hists = []\n",
    "        for slice in data[\"slices\"]:\n",
    "            mask = slice[\"corr\"][idx]> 0.03\n",
    "            data_masked = slice[\"corr\"][idx][mask]\n",
    "            hist_slice = np.histogram(data_masked, bins=bins)[0]\n",
    "            hists.append(hist_slice)\n",
    "        hists = np.array(hists)\n",
    "        mean_hist = np.mean(hists, axis=0)\n",
    "        std_hist = np.std(hists, axis=0)\n",
    "        \n",
    "        hists_shuffled = []\n",
    "        for slice in data_shuffled[\"slices\"]:\n",
    "            mask = slice[\"corr\"][idx]> 0.03\n",
    "            data_masked = slice[\"corr\"][idx][mask]\n",
    "            hist_slice = np.histogram(data_masked, bins=bins)[0]\n",
    "            hists_shuffled.append(hist_slice)\n",
    "        hists_shuffled = np.array(hists_shuffled)\n",
    "        mean_hist_shuffled = np.mean(hists_shuffled, axis=0)\n",
    "        std_hist_shuffled = np.std(hists_shuffled, axis=0)\n",
    "        ax.errorbar((bins[:-1] + bins[1:]) / 2.0, mean_hist, yerr=std_hist, color='blue', fmt='.')\n",
    "        ax.errorbar((bins[:-1] + bins[1:]) / 2.0, mean_hist_shuffled, yerr=std_hist_shuffled, color='red', fmt='.')\n",
    "\n",
    "        ax.set_xlabel(\"Correlation coefficient\")  # Add x-axis title\n",
    "        ax.set_ylabel(\"Number of neural firing pairs\")  # Add x-axis title\n",
    "        ax.set_title(f\"Correlation Histogram for {fn}\")  # Add title with fn\n",
    "        ax.legend()\n",
    "\n",
    "        fig_tmp = None\n",
    "        if fn in [\"UCSC_mouse_Pasca_23179\", \"O5_t_spk_mat_sorted\"]:\n",
    "            fig_tmp, ax_delta = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "        ax_delta.bar((bins[:-1] + bins[1:]) / 2.0, hist - hist_shuffled, (bins[1:] - bins[:-1]), color='green', alpha=0.5)\n",
    "        ax_delta.errorbar((bins[:-1] + bins[1:]) / 2.0, mean_hist - mean_hist_shuffled, yerr=std_hist + std_hist_shuffled, color='green', fmt='.')\n",
    "        ax_delta.set_xlabel(\"Correlation coefficient\", fontsize=20)\n",
    "        ax_delta.set_ylabel(\"Number of neural firing pairs\", fontsize=20)\n",
    "        ax_delta.set_title(f\"Correlation Histogram Delta for {fn}\", fontsize=20)\n",
    "        ax_delta.tick_params(axis='both', which='major', labelsize=18)\n",
    "        ax_delta.tick_params(axis='both', which='minor', labelsize=16)\n",
    "\n",
    "        if fig_tmp is not None:\n",
    "            ax_delta.set_title(\"\")\n",
    "            fig_tmp.savefig(f\"plots/correlation_delta_{fn}_lag_{lag}.png\")\n",
    "            plt.close(fig_tmp)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #trying to do log normal\n",
    "# ALL_FILES = [\n",
    "#     \"2950_spike_mat_or_rand\", #26\n",
    "#     \"2953_spike_mat_or_rand\", #Tal paper\n",
    "#     \"2957_spike_mat_or_rand\",\n",
    "#     \"5116_spike_mat_or_rand\",\n",
    "# ]\n",
    "\n",
    "# import numpy as np\n",
    "# import scipy.stats\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# for idx, fn in enumerate(ALL_FILES):\n",
    "#         with open(f\"processed/{fn}_lag_window_0.pkl\", \"rb\") as f:\n",
    "#             data = pickle.load(f)\n",
    "#         with open(f\"processed/{fn}_lag_window_0_shuffle.pkl\", \"rb\") as f:\n",
    "#             data_shuffled = pickle.load(f)\n",
    "\n",
    "#         # Extracting the upper triangle indices\n",
    "#         idx = np.triu_indices(data[\"corr\"].shape[0], 1)\n",
    "\n",
    "#         # Applying mask\n",
    "#         mask = data[\"corr\"][idx] > 0.03\n",
    "#         data_masked = data[\"corr\"][idx][mask]\n",
    "#         # Generate log-normal distributed set of sample\n",
    "\n",
    "#         # Make a fit to the samples.\n",
    "#         shape, loc, scale = scipy.stats.lognorm.fit(data_masked, floc=0)\n",
    "\n",
    "#         # Create the histogram plot using matplotlib.  The first two values in\n",
    "#         # the tuple returned by hist are the number of samples in each bin and\n",
    "#         # the values of the histogram's bin edges.  counts has length num_bins,\n",
    "#         # and edges has length num_bins + 1.\n",
    "#         num_bins = 50\n",
    "#         clr = '#FFE090'\n",
    "#         counts, edges, patches = plt.hist(data_masked, bins=num_bins, color=clr, ec='k', label='data_masked')\n",
    "\n",
    "#         # Create an array of length num_bins containing the center of each bin.\n",
    "#         centers = 0.5*(edges[:-1] + edges[1:])\n",
    "\n",
    "#         # Compute the CDF at the edges. Then prob, the array of differences,\n",
    "#         # is the probability of a sample being in the corresponding bin.\n",
    "#         cdf = scipy.stats.lognorm.cdf(edges, shape, loc=loc, scale=scale)\n",
    "#         prob = np.diff(cdf)\n",
    "\n",
    "#         plt.plot(centers, data_masked.size*prob, 'k-', linewidth=2, label='Expected histogram')\n",
    "\n",
    "#         # prob can also be approximated using the PDF at the centers multiplied\n",
    "#         # by the width of the bin:\n",
    "#         # p = scipy.stats.lognorm.pdf(centers, shape, loc=loc, scale=scale)\n",
    "#         # prob = p*(edges[1] - edges[0])\n",
    "#         # plt.plot(centers, samples.size*prob, 'r')\n",
    "\n",
    "#         plt.legend()\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #trying to do log normal\n",
    "# ALL_FILES = [\n",
    "#     \"2950_spike_mat_or_rand\", #26\n",
    "#     \"2953_spike_mat_or_rand\", #Tal paper\n",
    "#     \"2957_spike_mat_or_rand\",\n",
    "#     \"5116_spike_mat_or_rand\",\n",
    "# ]\n",
    "\n",
    "# import numpy as np\n",
    "# import scipy.stats\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# for idx, fn in enumerate(ALL_FILES):\n",
    "#         with open(f\"processed/{fn}_lag_window_0.pkl\", \"rb\") as f:\n",
    "#             data = pickle.load(f)\n",
    "#         with open(f\"processed/{fn}_lag_window_0_shuffle.pkl\", \"rb\") as f:\n",
    "#             data_shuffled = pickle.load(f)\n",
    "\n",
    "#         # Extracting the upper triangle indices\n",
    "#         idx = np.triu_indices(data[\"corr\"].shape[0], 1)\n",
    "\n",
    "#         # Applying mask\n",
    "#         mask = data[\"corr\"][idx] > 0.03\n",
    "#         data_masked = data[\"corr\"][idx][mask]\n",
    "#         # Generate log-normal distributed set of sample\n",
    "\n",
    "#         # Make a fit to the samples.\n",
    "#         shape, loc, scale = scipy.stats.lognorm.fit(data_masked, floc=0)\n",
    "\n",
    "#         # Create the histogram plot using matplotlib.  The first two values in\n",
    "#         # the tuple returned by hist are the number of samples in each bin and\n",
    "#         # the values of the histogram's bin edges.  counts has length num_bins,\n",
    "#         # and edges has length num_bins + 1.\n",
    "#         num_bins = 50\n",
    "#         clr = '#FFE090'\n",
    "#         counts, edges, patches = plt.hist(data_masked, bins=num_bins, color=clr, ec='k', label='data_masked')\n",
    "\n",
    "#         # Create an array of length num_bins containing the center of each bin.\n",
    "#         centers = 0.5*(edges[:-1] + edges[1:])\n",
    "\n",
    "#         # Compute the CDF at the edges. Then prob, the array of differences,\n",
    "#         # is the probability of a sample being in the corresponding bin.\n",
    "#         cdf = scipy.stats.lognorm.cdf(edges, shape, loc=loc, scale=scale)\n",
    "#         prob = np.diff(cdf)\n",
    "\n",
    "#         plt.plot(centers, data_masked.size*prob, 'k-', linewidth=2, label='Expected histogram')\n",
    "\n",
    "#         # prob can also be approximated using the PDF at the centers multiplied\n",
    "#         # by the width of the bin:\n",
    "#         # p = scipy.stats.lognorm.pdf(centers, shape, loc=loc, scale=scale)\n",
    "#         # prob = p*(edges[1] - edges[0])\n",
    "#         # plt.plot(centers, samples.size*prob, 'r')\n",
    "\n",
    "#         plt.legend()\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (no window)Take the highest correlations tail from step 1. and construct a network \n",
    "\n",
    "- node are neural firing rates (one node is 1-2-1 firing rate): one row of the initial matrix\n",
    "- edge are correlation coefficients \n",
    "\n",
    "Result will be four networks for each of highest correlated tail in the given data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (no window) Perform hole analyzis for the network from Step 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ripser import ripser\n",
    "from persim import plot_diagrams\n",
    "\n",
    "for lag in [0, 10, 20]:\n",
    "    fig, axs = plt.subplots((len(ALL_FILES) + 1) // 2, 2, figsize=(12, len(ALL_FILES) * 3))\n",
    "    fig.suptitle(f\"Lag window: {lag}\")\n",
    "    for idx, fn in enumerate(ALL_FILES):\n",
    "        with open(f\"processed/{fn}_lag_window_{lag}.pkl\", \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            data = data[\"all\"]\n",
    "        plot_diagrams(data[\"ripser\"][\"dgms\"], ax=axs[idx // 2, idx % 2], show=False, title=f\"{fn}\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Repeat step 1 and step 2 and step 3 for lag window 10 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for lag in [0, 10, 20]:\n",
    "    fig, axs = plt.subplots(len(ALL_FILES) // 2, 2, figsize=(12, len(ALL_FILES) * 3))\n",
    "    fig.suptitle(f\"Lag window: {lag}\")\n",
    "    for idx, fn in enumerate(ALL_FILES):\n",
    "        with open(f\"processed/{fn}_lag_window_{lag}.pkl\", \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            data_all = data[\"all\"]\n",
    "        with open(f\"processed/{fn}_lag_window_{lag}_shuffle.pkl\", \"rb\") as f:\n",
    "            data_shuffled = pickle.load(f)\n",
    "            data_shuffled_all = data_shuffled[\"all\"]\n",
    "        ax = axs[idx // 2, idx % 2]\n",
    "        ax.plot([len(h) for h in data_all[\"holes\"]], label=\"Original\")\n",
    "        ax.plot([len(h) for h in data_shuffled_all[\"holes\"]], label=\"Shuffled\")\n",
    "        ax.set_title(f\"Number of holes for {fn}\")\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(\"Number of nodes\")\n",
    "        ax.set_ylabel(\"Number of holes\")\n",
    "\n",
    "        data_slices = []\n",
    "        for slice in data[\"slices\"]:\n",
    "            data_slices.append([len(h) for h in slice[\"holes\"]])\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "for n in [4, 5, 6]:\n",
    "    for lag in [0, 10, 20]:\n",
    "        fig, axs = plt.subplots(len(ALL_FILES), 2, figsize=(12, len(ALL_FILES) * 6))\n",
    "        fig.suptitle(f\"Lag window: {lag}, hole size: {n}\")\n",
    "        for idx, fn in enumerate(ALL_FILES):\n",
    "            for shuffle in [False, True]:\n",
    "                with open(f\"processed/{fn}_lag_window_{lag}{'_shuffle' if shuffle else ''}.pkl\", \"rb\") as f:\n",
    "                    data = pickle.load(f)\n",
    "                    data = data[\"all\"]\n",
    "                ax = axs[idx, int(shuffle)]\n",
    "                if n >= len(data[\"holes\"]):\n",
    "                    continue\n",
    "                xs1, ys1 = zip(*data[\"holes\"][n]) if data[\"holes\"][n] else ([], [])\n",
    "                xy1 = np.vstack([xs1, ys1])\n",
    "                # print('here', xy1.shape)\n",
    "                # log_z1 = np.log(gaussian_kde(xy1)(xy1))\n",
    "                scatter = ax.scatter(xs1, ys1, s=0.01, alpha=0.1, vmax=10.0)\n",
    "                ax.set_xlabel(\"Birth correlation coefficient\")\n",
    "                ax.set_ylabel(\"Death correlation coefficient\")\n",
    "                ax.set_xlim(0, 1)\n",
    "                ax.set_ylim(0, 1)\n",
    "                ax.set_title(f\"{fn}{', shuffled' if shuffle else ''}\")\n",
    "            \n",
    "        fig.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Repeat step 1 and step 2 and step 3for lag window 20 ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for lag in [0, 10, 20]:\n",
    "    fig, axs = plt.subplots(len(ALL_FILES) // 2, 2, figsize=(12, len(ALL_FILES) * 3))\n",
    "    fig.suptitle(f\"Lag window: {lag}\")\n",
    "    for idx, fn in enumerate(ALL_FILES):\n",
    "        with open(f\"processed/{fn}_lag_window_{lag}.pkl\", \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            data = data[\"all\"]\n",
    "        with open(f\"processed/{fn}_lag_window_{lag}_shuffle.pkl\", \"rb\") as f:\n",
    "            data_shuffled = pickle.load(f)\n",
    "            data_shuffled = data_shuffled[\"all\"]\n",
    "        ax = axs[idx // 2, idx % 2]\n",
    "        ax.plot(data[\"cc_counts\"], label=\"Original\")\n",
    "        ax.plot(data_shuffled[\"cc_counts\"], label=\"Shuffled\")\n",
    "        ax.set_title(f\"Number of connected components for {fn}\")\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(\"Number of nodes\")\n",
    "        ax.set_ylabel(\"Number of connected components\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ripser import ripser\n",
    "from persim import plot_diagrams\n",
    "\n",
    "FILES = [\n",
    "    \"5116_spike_mat_or_rand\",\n",
    "    \"UCSC_mouse_Pasca_23179\",\n",
    "]\n",
    "# FILES = ALL_FILES\n",
    "\n",
    "def compute_live(dgm):\n",
    "    events = [(0.0, 0), (1.0, 0)]\n",
    "    if len(dgm) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    for birth, death in dgm:\n",
    "        events.append((birth, 1))\n",
    "        events.append((death, -1))\n",
    "    events.sort()\n",
    "    t, val = map(np.array, zip(*events))\n",
    "    val = np.cumsum(val)\n",
    "    return t, val\n",
    "\n",
    "for idx, fn in enumerate(FILES):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    for lag in [0, 10, 20]:\n",
    "        with open(f\"processed/{fn}_lag_window_{lag}.pkl\", \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            data = data[\"all\"]\n",
    "        for it in range(3):\n",
    "            ax = axs[it]\n",
    "            t, live_cnt = compute_live(data[\"ripser\"][\"dgms\"][it])\n",
    "            t = 1.0 - t\n",
    "            ax.step(t, live_cnt, label=f\"lag={lag}\")\n",
    "            # ax.set_title(f\"{fn}, H{it}\")\n",
    "            ax.set_xlabel(\"Correlation coefficient\", fontsize=20)\n",
    "            ax.set_ylabel(f\"Betti number b{it}\", fontsize=20)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "            ax.tick_params(axis='both', which='minor', labelsize=16)\n",
    "            ax.legend(fontsize=20)\n",
    "    # fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx, fn in enumerate(FILES):\n",
    "    print(\"Lag\\t\", end=\"\\t\")\n",
    "    for lag in [0, 10, 20]:\n",
    "        print(lag, end=\"\\t\")\n",
    "    print()\n",
    "    for r_min, r_max in [(0.5, 0.7), (0.7, 0.9), (0.9, 1.0)]:\n",
    "        print(f\"({r_min}, {r_max})\", end=\"\\t\")\n",
    "        for lag in [0, 10, 20]:\n",
    "            with open(f\"processed/{fn}_lag_window_{lag}.pkl\", \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "                data = data[\"all\"]\n",
    "            cnt = np.sum((data[\"corr\"] > r_min) & (data[\"corr\"] <= r_max))\n",
    "            print(cnt, end=\"\\t\")\n",
    "        print()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES = [\n",
    "    \"5116_spike_mat_or_rand\",\n",
    "]\n",
    "\n",
    "for idx, fn in enumerate(FILES):\n",
    "    fig, axs = plt.subplots(3, 3, figsize=(20, 18))\n",
    "    print(fn)\n",
    "    print(\"Lag\\t\", end=\"\\t\")\n",
    "    for lag in [0, 10, 20]:\n",
    "        print(lag, end=\"\\t\")\n",
    "    print()\n",
    "    for r_idx, (r_min, r_max) in enumerate([(0.5, 0.7), (0.7, 0.9), (0.9, 1.0)]):\n",
    "        print(f\"({r_min}, {r_max})\", end=\"\\t\")\n",
    "        for lag_idx, lag in enumerate([0, 10, 20]):\n",
    "            ax = axs[r_idx, lag_idx]\n",
    "            with open(f\"processed/{fn}_lag_window_{lag}.pkl\", \"rb\") as f:\n",
    "                data = pickle.load(f)\n",
    "                data = data[\"all\"]\n",
    "            cc_counts = data[\"range_analysis\"][(r_min, r_max)][\"cc_counts\"]\n",
    "            bins = np.arange(0, np.max(cc_counts) + 1.5) - 0.5\n",
    "            ax.hist(cc_counts, bins)\n",
    "            ax.set_title(f\"({r_min}, {r_max}), lag={lag}\")\n",
    "            ax.set_xlabel(\"Size of connected component\")\n",
    "            ax.set_ylabel(\"Number of connected components\")\n",
    "            cnt = len(cc_counts)\n",
    "            print(cnt, end=\"\\t\")\n",
    "        print()\n",
    "    fig.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ripser import ripser\n",
    "from persim import plot_diagrams\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "import os\n",
    "os.makedirs(\"barcodes\", exist_ok=True)\n",
    "\n",
    "# ALL_FILES = [\n",
    "#     \"5116_spike_mat_or_rand\",\n",
    "#     \"UCSC_mouse_Pasca_23149\",\n",
    "# ]\n",
    "\n",
    "for lag in [0, 10, 20]:\n",
    "    for idx, fn in enumerate(ALL_FILES):\n",
    "        with open(f\"processed/{fn}_lag_window_{lag}.pkl\", \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            data = data[\"all\"]\n",
    "            dgms = data[\"ripser\"][\"dgms\"]\n",
    "        fig, axs = plt.subplots(3, 1, figsize=(12, 9), sharex=True)\n",
    "        # fig.suptitle(f\"Lag window: {lag} for {fn}\", fontsize=20)\n",
    "        for d in range(3):\n",
    "            ax = axs[d]\n",
    "            dgm = 1.0 - data[\"ripser\"][\"dgms\"][d]\n",
    "            for i in range(len(dgm)):\n",
    "                ax.plot([dgm[i][0], dgm[i][1]], [i + 1, i + 1], f'C{d}-', lw=2)\n",
    "            ax.set_ylabel(f\"$H_{d}$\", fontsize=20)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "            ax.tick_params(axis='both', which='minor', labelsize=16)\n",
    "            ax.set_ylim(0.5, max(2.5, len(dgm) + 0.5))\n",
    "            ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        axs[-1].set_xlabel(\"correlation coefficient\", fontsize=20)\n",
    "        lines = {\n",
    "            \"UCSC_mouse_Pasca_23149\": {10: 0.72, 20: 0.72, 0: 0.765},\n",
    "            \"5116_spike_mat_or_rand\": {10: 0.25, 0: 0.20, 20: 0.45},\n",
    "        }\n",
    "        if fn in lines and lag in lines[fn]:\n",
    "            for ax in axs:\n",
    "                ax.axvline(lines[fn][lag], color='red')\n",
    "            bs = []\n",
    "            for d in range(3):\n",
    "                dgm = 1.0 - data[\"ripser\"][\"dgms\"][d]\n",
    "                bs.append(len([b for d, b in dgm if b < lines[fn][lag] < d]))\n",
    "            bs = f\"({bs[0]}, {bs[1]}, {bs[2]})\"\n",
    "            bs = f\"Betti numbers: {bs}\"\n",
    "            fig.text(0.5, 0.10, bs, ha='center', fontsize=20)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f\"barcodes/ripser_{fn}_{lag}.png\")\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['t_spk_mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lag in [0, 10, 20, 100, 200]:\n",
    "    fig, axs = plt.subplots((len(ALL_FILES) + 1) // 2, 2, figsize=(12, len(ALL_FILES) * 3))\n",
    "    if len(axs.shape) == 1:\n",
    "        axs = axs[None, :]\n",
    "    fig.suptitle(f\"Lag window: {lag}\")\n",
    "    for idx, fn in enumerate(ALL_FILES):\n",
    "        try:\n",
    "            data = loadmat(f\"{fn}.mat\")\n",
    "            xy = data['xy_raw']\n",
    "        except:\n",
    "            data = h5py.File(f\"{fn}.mat\")\n",
    "            xy = np.array(data['xy_raw']).T\n",
    "\n",
    "        with open(f\"processed/{fn}_lag_window_{lag}.pkl\", \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            data = data[\"all\"]\n",
    "        \n",
    "        ax = axs[idx // 2, idx % 2]\n",
    "        ax.scatter(xy[:, 0], xy[:, 1], s=2)\n",
    "        ax.set_title(f\"{fn}\")\n",
    "        if not 'ripserer' in data:\n",
    "            continue\n",
    "        for d in range(2, len(data['ripserer']) + 2):\n",
    "            cycles = data['ripserer'][d - 2]\n",
    "            if len(cycles):\n",
    "                assert d in [2, 3]\n",
    "                for birth, death, cycle in cycles:\n",
    "                    c = np.random.rand(3,)\n",
    "                    for r in cycle:\n",
    "                        if d == 2:\n",
    "                            ax.plot([xy[r[0], 0], xy[r[1], 0]], [xy[r[0], 1], xy[r[1], 1]], '-', c=c, lw=0.5)\n",
    "                        elif d == 3:\n",
    "                            ax.fill(xy[r, 0], xy[r, 1], color=c, alpha=0.2)\n",
    "        # for d in range(1, len(data['ripser']['cocycles']) + 1):\n",
    "        #     cocycles = data['ripser']['cocycles'][d - 1]\n",
    "        #     if len(cocycles):\n",
    "        #         assert d in [2, 3]\n",
    "        #         for cocycle in cocycles:\n",
    "        #             for r in cocycle:\n",
    "        #                 if d == 2:\n",
    "        #                     ax.plot([xy[r[0], 0], xy[r[1], 0]], [xy[r[0], 1], xy[r[1], 1]], 'g-')\n",
    "        #                 elif d == 3:\n",
    "        #                     t = plt.Polygon(xy[r[:-1]], color='b', fill=True, alpha = 0.3)\n",
    "        #                     ax.add_patch(t)\n",
    "    plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
